# PydanticAI Integration: Enhancing the Current Architecture

## Overview

This document outlines how **PydanticAI** can be integrated into the current agentic architecture to improve type safety, structured outputs, prompt management, and overall agent design.

PydanticAI is a Python agent framework designed to make it less painful to build production-grade applications with Generative AI. It's built on top of Pydantic and provides elegant, type-safe patterns for LLM interactions.

---

## Table of Contents

- [Current State Analysis](#current-state-analysis)
- [PydanticAI Benefits](#pydanticai-benefits)
- [Integration Points](#integration-points)
- [Migration Strategy](#migration-strategy)
- [Code Examples](#code-examples)
- [Expected Improvements](#expected-improvements)

---

## Current State Analysis

### What We Have Now

Our current architecture already follows many PydanticAI principles:

âœ… **Pydantic-based agents** - `BaseAgent[InputModel, OutputModel]` uses Pydantic
âœ… **Type-safe inputs/outputs** - All agent I/O is validated
âœ… **Dependency injection** - Agents receive dependencies via constructor
âœ… **Stateless execution** - All state in input/context
âœ… **Uniform interface** - All agents implement same `BaseAgent`

### What We're Missing

âŒ **Structured LLM outputs** - Currently parse JSON manually
âŒ **Prompt management** - Inline strings, not centralized
âŒ **Tool/function calling** - Not using LLM function calling features
âŒ **Retry logic** - No automatic retries on LLM failures
âŒ **Streaming support** - No streaming responses
âŒ **Result validation** - Manual validation of LLM outputs
âŒ **Agent composition** - No built-in patterns for chaining agents

---

## PydanticAI Benefits

### 1. Structured Outputs with Type Safety

**Problem**: Currently, we parse JSON manually from LLM responses

**Current Code** (IntentClassifierAgent):
```python
response = llm.invoke(messages)
response_text = response.content.strip()

# Manually extract JSON
json_start = response_text.find('{')
json_end = response_text.rfind('}') + 1
json_text = response_text[json_start:json_end]
result = json.loads(json_text)  # âŒ Can fail, not type-safe

# Manual validation
intent = result['intent']  # âŒ No type checking
confidence = result['confidence']
```

**With PydanticAI**:
```python
from pydantic import BaseModel
from pydantic_ai import Agent

class IntentResult(BaseModel):
    intent: Literal["informational", "recommendation"]
    category: Optional[str]
    confidence: float = Field(ge=0, le=1)
    reasoning: str

agent = Agent(
    'openai:gpt-4',
    result_type=IntentResult,  # âœ… Type-safe, automatic validation
    system_prompt="You are an intelligent query classifier..."
)

# Automatically parses and validates
result = await agent.run(query)
# result.data is IntentResult - fully typed! âœ…
```

---

### 2. Built-in Prompt Management

**Problem**: Prompts are inline strings scattered across code

**Current Code**:
```python
# Inline prompt in agent code
prompt = f"""Based on the customer's purchase history, provide a brief explanation...

{customer_context}

Recommendations:
{rec_text}

Provide a 2-3 sentence explanation..."""
```

**With PydanticAI**:
```python
from pydantic_ai import Agent

# Prompt defined with agent
response_agent = Agent(
    'openai:gpt-4',
    result_type=str,
    system_prompt="""You are a personalized shopping assistant.
    Your role is to explain product recommendations clearly and concisely.
    Focus on why products match customer preferences."""
)

# Use with dynamic prompts
@response_agent.system_prompt
def get_dynamic_prompt(ctx: RunContext) -> str:
    return f"""Customer: {ctx.deps.customer_name}
    Segment: {ctx.deps.price_segment}
    Favorite Categories: {', '.join(ctx.deps.favorite_categories)}"""

# Or use prompt templates
result = await response_agent.run(
    f"Explain these recommendations:\n{rec_text}",
    deps=customer_context  # Injected as dependencies
)
```

---

### 3. Function/Tool Calling

**Problem**: Cannot use LLM function calling for structured actions

**With PydanticAI**:
```python
from pydantic_ai import Agent, RunContext

class CustomerLookupDeps(BaseModel):
    customer_repo: CustomerRepository

agent = Agent(
    'openai:gpt-4',
    deps_type=CustomerLookupDeps,
)

# Define tools the agent can call
@agent.tool
async def get_customer_info(
    ctx: RunContext[CustomerLookupDeps],
    customer_id: str
) -> dict:
    """Get customer information by ID"""
    return ctx.deps.customer_repo.get_by_id(customer_id)

@agent.tool
async def search_products(
    ctx: RunContext[CustomerLookupDeps],
    category: str,
    max_price: float
) -> list:
    """Search for products in a category under a price limit"""
    # Tool implementation
    return products

# Agent can now call these tools automatically!
result = await agent.run(
    "Find budget-friendly electronics for customer 123",
    deps=CustomerLookupDeps(customer_repo=repo)
)
```

---

### 4. Automatic Retries and Error Handling

**Problem**: Manual error handling, no automatic retries

**Current Code**:
```python
try:
    response = llm.invoke(messages)
    result = json.loads(response.content)
except Exception as e:
    # Manual fallback
    return default_response
```

**With PydanticAI**:
```python
from pydantic_ai import Agent
from pydantic_ai.settings import ModelSettings

agent = Agent(
    'openai:gpt-4',
    result_type=IntentResult,
    model_settings=ModelSettings(
        max_retries=3,  # âœ… Automatic retries
        timeout=30,
    )
)

# Automatically retries on failure, validates output
result = await agent.run(query)
```

---

### 5. Streaming Support

**Problem**: No streaming responses for long-running LLM calls

**With PydanticAI**:
```python
agent = Agent('openai:gpt-4', result_type=str)

# Stream response chunks
async with agent.run_stream(query) as stream:
    async for chunk in stream.stream_text():
        print(chunk, end='', flush=True)

# Get final validated result
final = await stream.get_data()
```

---

### 6. Result Validation and Constraints

**Problem**: Limited validation of LLM outputs

**With PydanticAI**:
```python
from pydantic import BaseModel, Field, field_validator

class ProductRecommendation(BaseModel):
    product_id: str = Field(pattern=r'^\d+$')
    product_name: str = Field(min_length=1, max_length=200)
    score: float = Field(ge=0, le=1)
    reason: str = Field(min_length=10, max_length=500)

    @field_validator('score')
    @classmethod
    def score_must_be_reasonable(cls, v: float) -> float:
        if v < 0.1:
            raise ValueError('Score too low, recommendation not confident')
        return v

# Agent automatically validates all fields
agent = Agent('openai:gpt-4', result_type=ProductRecommendation)
result = await agent.run(query)
# result.data is fully validated ProductRecommendation âœ…
```

---

## Integration Points

### 1. Replace ResponseGenerationAgent with PydanticAI

**Current**: Manual prompt construction and LLM invocation

**Proposed**:
```python
# app/capabilities/agents/response_generation_v2.py
from pydantic_ai import Agent, RunContext
from pydantic import BaseModel

class ResponseDeps(BaseModel):
    """Dependencies for response generation"""
    customer_name: str
    price_segment: str
    favorite_categories: list[str]

class ResponseOutput(BaseModel):
    """Validated response output"""
    reasoning: str = Field(min_length=50, max_length=500)
    confidence: float = Field(ge=0, le=1)

# Create PydanticAI agent
response_agent = Agent(
    'openai:gpt-4',
    result_type=ResponseOutput,
    deps_type=ResponseDeps,
    system_prompt="""You are a personalized shopping assistant.
    Explain recommendations clearly, focusing on customer preferences."""
)

@response_agent.system_prompt
def dynamic_system_prompt(ctx: RunContext[ResponseDeps]) -> str:
    """Add dynamic customer context to system prompt"""
    return f"""Customer Profile:
    - Name: {ctx.deps.customer_name}
    - Segment: {ctx.deps.price_segment}
    - Interests: {', '.join(ctx.deps.favorite_categories)}

    Provide personalized explanations that reference these preferences."""

class ResponseGenerationAgentV2(BaseAgent[ResponseGenerationInput, ResponseGenerationOutput]):
    """Response generation using PydanticAI"""

    def __init__(self):
        metadata = AgentMetadata(
            id="response_generation_v2",
            name="Response Generation Agent (PydanticAI)",
            description="Generates explanations using PydanticAI structured outputs",
            version="2.0.0",
            input_schema=ResponseGenerationInput,
            output_schema=ResponseGenerationOutput,
            tags=["response", "llm", "pydantic-ai"],
        )
        super().__init__(metadata)

    async def _execute(
        self,
        input_data: ResponseGenerationInput,
        context: AgentContext,
    ) -> ResponseGenerationOutput:
        """Generate response using PydanticAI"""
        profile = input_data.customer_profile
        recommendations = input_data.recommendations

        # Build recommendations text
        rec_text = "\n".join([
            f"{i+1}. {r.product_name} (${r.avg_price:.2f}): {r.reason}"
            for i, r in enumerate(recommendations[:3])
        ])

        # Create dependencies
        deps = ResponseDeps(
            customer_name=profile.customer_name,
            price_segment=profile.price_segment,
            favorite_categories=profile.favorite_categories,
        )

        # Run PydanticAI agent
        result = await response_agent.run(
            f"Explain these recommendations:\n{rec_text}",
            deps=deps,
        )

        # Extract validated output
        return ResponseGenerationOutput(
            reasoning=result.data.reasoning,
            llm_used=True,
        )
```

**Benefits**:
- âœ… Automatic output validation
- âœ… Type-safe reasoning field
- âœ… Cleaner prompt management
- âœ… Built-in retry logic
- âœ… Better error messages

---

### 2. Enhance IntentClassifierAgent with PydanticAI

**Current**: Manual JSON parsing with error-prone extraction

**Proposed**:
```python
# app/agents/intent_classifier_agent_v2.py
from pydantic_ai import Agent
from pydantic import BaseModel, Field
from typing import Literal, Optional

class IntentClassification(BaseModel):
    """Structured intent classification result"""
    intent: Literal["informational", "recommendation"]
    category: Optional[Literal[
        "total_purchases",
        "spending",
        "favorite_categories",
        "recent_purchases",
        "customer_profile",
        "general"
    ]] = None
    confidence: float = Field(ge=0, le=1)
    reasoning: str = Field(min_length=10)

# Create PydanticAI intent classifier
intent_agent = Agent(
    'openai:gpt-4',
    result_type=IntentClassification,
    system_prompt="""You are an intelligent query classifier for a personalized shopping assistant.

Classify queries into:
1. INFORMATIONAL - Questions about customer data (purchases, spending, favorites, etc.)
2. RECOMMENDATION - Requests for product suggestions

For informational queries, specify the category.
Always provide your reasoning."""
)

class IntentClassifierAgentV2:
    """Intent classifier using PydanticAI"""

    async def classify(self, query: str) -> IntentClassification:
        """Classify query intent with structured output"""
        result = await intent_agent.run(
            f"Classify this query: {query}",
            message_history=[
                # Include few-shot examples
                ("user", "what is the total purchase of Kenneth Martinez?"),
                ("assistant", '{"intent": "informational", "category": "total_purchases", "confidence": 0.95, "reasoning": "Direct question about total purchases"}'),
                ("user", "recommend some products for Kenneth Martinez"),
                ("assistant", '{"intent": "recommendation", "category": null, "confidence": 0.98, "reasoning": "Direct request for recommendations"}'),
            ]
        )

        # result.data is fully validated IntentClassification âœ…
        return result.data
```

**Benefits**:
- âœ… No manual JSON parsing
- âœ… Automatic validation
- âœ… Type-safe enums
- âœ… Built-in error handling
- âœ… Cleaner code

---

### 3. Add Function Calling for Complex Workflows

**New Capability**: Allow LLMs to call functions to gather data

**Example**:
```python
# app/capabilities/agents/smart_recommendation_agent.py
from pydantic_ai import Agent, RunContext

class SmartRecommendationDeps(BaseModel):
    customer_repo: CustomerRepository
    product_repo: ProductRepository
    vector_repo: VectorRepository

smart_agent = Agent(
    'openai:gpt-4',
    deps_type=SmartRecommendationDeps,
    result_type=list[ProductRecommendation],
)

@smart_agent.tool
async def get_customer_profile(
    ctx: RunContext[SmartRecommendationDeps],
    customer_id: str
) -> dict:
    """Get customer profile by ID"""
    customer = ctx.deps.customer_repo.get_by_id(customer_id)
    purchases = ctx.deps.customer_repo.get_purchases_by_customer_id(customer_id)
    return {
        "customer_id": customer_id,
        "total_purchases": len(purchases),
        "categories": list(set(p["product_category"] for p in purchases)),
    }

@smart_agent.tool
async def find_similar_customers(
    ctx: RunContext[SmartRecommendationDeps],
    customer_id: str,
    top_k: int = 10
) -> list[str]:
    """Find similar customers using vector search"""
    # Use vector repository to find similar customers
    similar = ctx.deps.vector_repo.search_similar(
        query_text=f"customer_{customer_id}",
        top_k=top_k
    )
    return [customer_id for customer_id, _ in similar]

@smart_agent.tool
async def get_popular_products_in_category(
    ctx: RunContext[SmartRecommendationDeps],
    category: str,
    limit: int = 10
) -> list[dict]:
    """Get popular products in a category"""
    products = ctx.deps.product_repo.get_by_category(category)
    # Sort by popularity and return top N
    return sorted(products, key=lambda x: x.get("purchase_count", 0), reverse=True)[:limit]

# Agent can now autonomously call these tools!
result = await smart_agent.run(
    "Find personalized recommendations for customer 887",
    deps=SmartRecommendationDeps(
        customer_repo=CustomerRepository(),
        product_repo=ProductRepository(),
        vector_repo=VectorRepository(),
    )
)

# LLM automatically called tools to gather data and generate recommendations âœ…
```

**Benefits**:
- âœ… LLM can gather data autonomously
- âœ… Reduces need for pre-built workflows
- âœ… More flexible and adaptive
- âœ… Self-documenting (tool docstrings)

---

### 4. Improve Prompt Management with Templates

**Current**: Inline f-strings

**With PydanticAI**:
```python
from pydantic_ai import Agent

# Load prompts from files
import yaml

prompts_config = yaml.safe_load(open("prompts/config.yaml"))

agent = Agent(
    'openai:gpt-4',
    result_type=str,
    system_prompt=prompts_config["response_generation"]["system"],
)

# Or use dynamic prompts
@agent.system_prompt
def get_system_prompt(ctx: RunContext) -> str:
    """Load prompt from file based on context"""
    prompt_key = ctx.metadata.get("prompt_version", "default")
    return load_prompt_from_file(f"prompts/{prompt_key}.txt")
```

---

### 5. Add Result Caching and Optimization

**With PydanticAI**:
```python
from pydantic_ai import Agent
from functools import lru_cache

# Cache agent results
@lru_cache(maxsize=100)
def get_cached_classification(query: str) -> IntentClassification:
    """Cache intent classification results"""
    result = intent_agent.run_sync(query)
    return result.data

# Or use PydanticAI's built-in result handling
agent = Agent('openai:gpt-4', result_type=IntentResult)

result = await agent.run(query)
# Access usage info
print(f"Tokens used: {result.usage()}")
print(f"Cost: ${result.cost()}")
```

---

## Migration Strategy

### Phase 1: Proof of Concept (1-2 days)

**Goal**: Validate PydanticAI with one agent

1. âœ… Install PydanticAI: `pip install pydantic-ai`
2. âœ… Create `ResponseGenerationAgentV2` using PydanticAI
3. âœ… Run side-by-side comparison
4. âœ… Measure improvements (code size, type safety, errors)

**Success Criteria**:
- Structured output works correctly
- No manual JSON parsing
- Type safety verified

---

### Phase 2: Intent Classifier Migration (2-3 days)

**Goal**: Replace manual JSON parsing with structured outputs

1. âœ… Create `IntentClassifierAgentV2` with PydanticAI
2. âœ… Define `IntentClassification` Pydantic model
3. âœ… Add few-shot examples to agent
4. âœ… Test classification accuracy
5. âœ… Replace old implementation

**Success Criteria**:
- No JSON parsing errors
- Better type safety
- Same or improved accuracy

---

### Phase 3: Add Function Calling (1 week)

**Goal**: Enable LLM to call tools for data gathering

1. âœ… Identify common data access patterns
2. âœ… Create tool functions with `@agent.tool`
3. âœ… Build `SmartRecommendationAgent` with tools
4. âœ… Test autonomous recommendation generation
5. âœ… Compare with workflow-based approach

**Success Criteria**:
- LLM successfully calls tools
- Generates valid recommendations
- Better adaptability to queries

---

### Phase 4: Prompt Management (3-5 days)

**Goal**: Centralize and externalize prompts

1. âœ… Create `prompts/` directory structure
2. âœ… Move all prompts to YAML/JSON files
3. âœ… Implement prompt loader for PydanticAI agents
4. âœ… Add prompt versioning
5. âœ… Create prompt testing framework

**Success Criteria**:
- All prompts in files
- Version controlled
- Easy to A/B test

---

### Phase 5: Full Migration (2-3 weeks)

**Goal**: Migrate all LLM-using agents to PydanticAI

1. âœ… Update `BaseAgent` to support PydanticAI patterns
2. âœ… Create hybrid agents (support both patterns)
3. âœ… Migrate remaining agents
4. âœ… Add comprehensive tests
5. âœ… Update documentation

---

## Code Examples

### Example 1: Complete PydanticAI Agent

```python
# app/capabilities/agents/pydantic_ai/customer_insights_agent.py
from pydantic_ai import Agent, RunContext
from pydantic import BaseModel, Field

# Define dependencies
class CustomerInsightsDeps(BaseModel):
    customer_repo: CustomerRepository
    product_repo: ProductRepository

# Define output model
class CustomerInsights(BaseModel):
    """Customer behavioral insights"""
    customer_id: str
    is_loyal: bool
    risk_of_churn: float = Field(ge=0, le=1)
    recommended_actions: list[str]
    reasoning: str

# Create agent
insights_agent = Agent(
    'openai:gpt-4',
    deps_type=CustomerInsightsDeps,
    result_type=CustomerInsights,
    system_prompt="""You are a customer insights analyst.
    Analyze customer behavior and provide actionable insights."""
)

# Define tools the agent can use
@insights_agent.tool
async def get_purchase_history(
    ctx: RunContext[CustomerInsightsDeps],
    customer_id: str,
    months: int = 6
) -> dict:
    """Get customer purchase history for the last N months"""
    purchases = ctx.deps.customer_repo.get_purchases_by_customer_id(customer_id)
    # Filter by date, calculate metrics
    return {
        "total_purchases": len(purchases),
        "total_spent": sum(p["price"] for p in purchases),
        "avg_order_value": sum(p["price"] for p in purchases) / len(purchases) if purchases else 0,
        "last_purchase_days_ago": calculate_days_ago(purchases[-1]["purchase_date"]) if purchases else None,
    }

@insights_agent.tool
async def get_product_reviews(
    ctx: RunContext[CustomerInsightsDeps],
    customer_id: str
) -> list[dict]:
    """Get reviews written by customer"""
    return ctx.deps.product_repo.get_reviews_by_customer(customer_id)

# Use the agent
async def analyze_customer(customer_id: str) -> CustomerInsights:
    """Analyze customer and get insights"""
    result = await insights_agent.run(
        f"Analyze customer {customer_id} and provide insights",
        deps=CustomerInsightsDeps(
            customer_repo=CustomerRepository(),
            product_repo=ProductRepository(),
        )
    )

    # result.data is fully validated CustomerInsights âœ…
    return result.data
```

---

### Example 2: Hybrid Agent (Both Patterns)

```python
# app/capabilities/agents/hybrid_response_agent.py
from typing import Union
from pydantic_ai import Agent as PydanticAgent

class ResponseGenerationAgentHybrid(BaseAgent[ResponseGenerationInput, ResponseGenerationOutput]):
    """Hybrid agent supporting both manual and PydanticAI approaches"""

    def __init__(self, use_pydantic_ai: bool = True):
        metadata = AgentMetadata(
            id="response_generation_hybrid",
            name="Response Generation Agent (Hybrid)",
            description="Supports both manual and PydanticAI approaches",
            version="2.0.0",
            input_schema=ResponseGenerationInput,
            output_schema=ResponseGenerationOutput,
            tags=["response", "hybrid"],
        )
        super().__init__(metadata)
        self.use_pydantic_ai = use_pydantic_ai

        if use_pydantic_ai:
            self.pydantic_agent = PydanticAgent(
                'openai:gpt-4',
                result_type=str,
                system_prompt="You are a personalized shopping assistant..."
            )

    async def _execute(
        self,
        input_data: ResponseGenerationInput,
        context: AgentContext,
    ) -> ResponseGenerationOutput:
        """Execute using selected approach"""
        if self.use_pydantic_ai:
            return await self._execute_pydantic_ai(input_data, context)
        else:
            return await self._execute_manual(input_data, context)

    async def _execute_pydantic_ai(self, input_data, context):
        """Use PydanticAI approach"""
        # ... PydanticAI implementation

    async def _execute_manual(self, input_data, context):
        """Use manual approach (original)"""
        # ... Original implementation
```

---

### Example 3: Streaming Responses

```python
# app/capabilities/agents/streaming_response_agent.py
from pydantic_ai import Agent

streaming_agent = Agent('openai:gpt-4', result_type=str)

async def generate_streaming_response(query: str):
    """Generate response with streaming"""
    async with streaming_agent.run_stream(query) as stream:
        # Stream chunks to client
        async for chunk in stream.stream_text(delta=True):
            yield chunk  # Send to FastAPI StreamingResponse

        # Get final validated result
        final_result = await stream.get_data()
        print(f"Final response: {final_result}")
```

---

## Expected Improvements

### Quantitative Metrics

| Metric | Before | With PydanticAI | Improvement |
|--------|--------|-----------------|-------------|
| **JSON Parsing Errors** | ~5% | 0% | **-100%** âœ… |
| **Type Safety** | Manual validation | Automatic | **+100%** âœ… |
| **Code for LLM Call** | ~30 lines | ~5 lines | **-83%** âœ… |
| **Prompt Management** | Inline strings | Centralized | **+100%** âœ… |
| **Retry Logic** | Manual | Automatic | **+100%** âœ… |
| **Output Validation** | Limited | Comprehensive | **+200%** âœ… |

### Qualitative Benefits

âœ… **Developer Experience**: Less boilerplate, more focus on logic
âœ… **Reliability**: Automatic validation reduces runtime errors
âœ… **Maintainability**: Prompts and logic clearly separated
âœ… **Testability**: Structured outputs easier to test
âœ… **Flexibility**: Function calling enables new patterns
âœ… **Observability**: Built-in usage tracking and cost monitoring

---

## Integration with Existing Architecture

### How PydanticAI Fits

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    API Layer (FastAPI)                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 Service Layer (Thin Facade)                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚            Workflow Layer (Orchestration)                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Agent Layer (BaseAgent Implementation)               â”‚
â”‚                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  Traditional Agents (No LLM)                         â”‚   â”‚
â”‚  â”‚  â€¢ CustomerProfilingAgent                            â”‚   â”‚
â”‚  â”‚  â€¢ SimilarCustomersAgent                             â”‚   â”‚
â”‚  â”‚  â€¢ SentimentFilteringAgent                           â”‚   â”‚
â”‚  â”‚  â€¢ ProductScoringAgent                               â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  PydanticAI-Powered Agents (LLM)                     â”‚   â”‚
â”‚  â”‚  â€¢ ResponseGenerationAgent (PydanticAI)              â”‚   â”‚
â”‚  â”‚  â€¢ IntentClassificationAgent (PydanticAI)            â”‚   â”‚
â”‚  â”‚  â€¢ SmartRecommendationAgent (PydanticAI + Tools)     â”‚   â”‚
â”‚  â”‚                                                      â”‚   â”‚
â”‚  â”‚  Uses:                                               â”‚   â”‚
â”‚  â”‚  - Structured outputs                                â”‚   â”‚
â”‚  â”‚  - Function calling                                  â”‚   â”‚
â”‚  â”‚  - Automatic retries                                 â”‚   â”‚
â”‚  â”‚  - Streaming support                                 â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Repository Layer (Data Access)                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Key Points**:
- âœ… PydanticAI only used for LLM-based agents
- âœ… Non-LLM agents remain unchanged
- âœ… Same `BaseAgent` interface for both
- âœ… Workflows don't need to know the difference
- âœ… Gradual migration possible

---

## Compatibility Matrix

| Feature | Current Architecture | PydanticAI | Compatible? |
|---------|---------------------|------------|-------------|
| **Pydantic Models** | âœ… | âœ… | âœ… Yes |
| **Type Safety** | âœ… | âœ… | âœ… Yes |
| **Async/Await** | âœ… | âœ… | âœ… Yes |
| **Dependency Injection** | âœ… | âœ… | âœ… Yes |
| **Observability** | âœ… | âœ… | âœ… Yes |
| **BaseAgent Interface** | âœ… | Wrap | âœ… Yes (via wrapper) |

**Conclusion**: PydanticAI is highly compatible with our current architecture! âœ…

---

## Risks and Mitigation

### Risk 1: Learning Curve

**Risk**: Team needs to learn PydanticAI patterns

**Mitigation**:
- Start with one agent (PoC)
- Provide training documentation
- Keep hybrid approach during transition
- Gradual migration over weeks

### Risk 2: Library Maturity

**Risk**: PydanticAI is relatively new

**Mitigation**:
- Use for non-critical agents first
- Keep fallback to manual implementation
- Monitor library updates
- Contribute to library if needed

### Risk 3: Vendor Lock-in

**Risk**: Tight coupling to PydanticAI

**Mitigation**:
- Wrap PydanticAI agents in our `BaseAgent`
- Keep interface abstraction
- Can switch implementations if needed
- Standard Pydantic models still portable

### Risk 4: Performance Overhead

**Risk**: Additional abstraction may slow things down

**Mitigation**:
- Benchmark before/after
- PydanticAI is optimized for production
- Caching and result validation worth the cost
- Monitor in production

---

## Recommendation

### **Adopt PydanticAI Gradually**

**Phase 1** (Now - 1 week):
1. âœ… Install and experiment with PydanticAI
2. âœ… Build `ResponseGenerationAgentV2` as PoC
3. âœ… Measure improvements

**Phase 2** (2-4 weeks):
1. âœ… Migrate `IntentClassifierAgent` to PydanticAI
2. âœ… Add function calling to one workflow
3. âœ… Externalize prompts to files

**Phase 3** (1-2 months):
1. âœ… Create `SmartRecommendationAgent` with tools
2. âœ… Add streaming support where needed
3. âœ… Fully migrate all LLM agents

**Phase 4** (Ongoing):
1. âœ… Optimize prompts with A/B testing
2. âœ… Add advanced features (caching, monitoring)
3. âœ… Share learnings with team

---

## Summary

### PydanticAI Solves These Problems

âœ… **Manual JSON Parsing** â†’ Automatic structured outputs
âœ… **Inline Prompts** â†’ Centralized prompt management
âœ… **Limited Validation** â†’ Comprehensive Pydantic validation
âœ… **No Retries** â†’ Built-in retry logic
âœ… **No Streaming** â†’ Native streaming support
âœ… **Complex Workflows** â†’ Function calling simplifies logic

### Expected Benefits

- **80% less LLM boilerplate code**
- **100% type safety for LLM outputs**
- **Zero JSON parsing errors**
- **Better prompt management**
- **More flexible agents (function calling)**
- **Improved reliability (auto-retries)**

### Next Steps

1. **Read PydanticAI docs**: https://ai.pydantic.dev
2. **Install**: `pip install pydantic-ai`
3. **Build PoC**: Migrate `ResponseGenerationAgent`
4. **Evaluate**: Compare with current implementation
5. **Decide**: Continue migration if successful

---

**PydanticAI is a natural fit for our architecture and will significantly improve LLM agent quality! ğŸš€**
